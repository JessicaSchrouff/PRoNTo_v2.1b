<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of prt_rvr</title>
  <meta name="keywords" content="prt_rvr">
  <meta name="description" content="Optimisation for Relevance Vector Regression">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../index.html">Home</a> &gt;  <a href="#">machines</a> &gt; prt_rvr.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../index.html"><img alt="<" border="0" src="../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for ./machines&nbsp;<img alt=">" border="0" src="../right.png"></a></td></tr></table>-->

<h1>prt_rvr
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong>Optimisation for Relevance Vector Regression</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong>function [varargout] = prt_rvr(varargin) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Optimisation for Relevance Vector Regression

 [w,alpha,beta,ll] = prt_rvr(Phi,t)
 Phi   - MxM matrix derived from kernel function of vector pairs
 t     - the values to be matched
 w     - weights
 alpha - 1/variance for the prior part of the model
 beta  - 1/variance for the likelihood part of the model
 ll    - the negative log-likelihood.

 [w,alpha,beta,nu,ll]=spm_rvr(K,t,opt)
 K     - a cell-array of MxM dot-product matrices.
 t     - the values to be matched
 opt   - either 'Linear' or 'Gaussian RBF'
         'Linear'       is for linear regression models, where
                        the optimal kernel is generated by
                        [nu(1)*K{1} + nu(1)*K{2}... ones(size(K{1},1),1)]
         'Gaussian RBF' is for regression using Gaussian radial basis
                        functions.  The kernel is generated from
                        P1  = nu(1)*K{1} + nu(1)*K{2} ... ;
                        P2  = repmat(diag(P1) ,1,size(P1,2)) +...
                              repmat(diag(P1)',size(P1,1),1) - 2*P1;
                        Phi = exp([-0.5*P2 ones(size(P1,1),1)]);
 w     - weights
 alpha - 1/variance for the prior part of the model
 beta  - 1/variance for the likelihood part of the model
 nu    - parameters that convert the dot-product matrices into
         a kernel matrix (Phi).
 ll    - the negative log-likelihood.

 The first way of calling the routine simply optimises the
 weights.  This involves estimating a restricted maximum
 likelihood (REML) solution, which maximises P(alpha,beta|t,Phi).
 Note that REML is also known as Type II Maximum Likelihood
 (ML-II). The ML-II solution tends towards infinite weights for
 some the regularisation terms (i.e. 1/alpha(i) approaches 0).
 The appropriate columns are removed from the model when
 this happens.

 The second way of calling the routine also estimates additional
 input scale parameters as described in Appendix C of Tipping (2001).
 This method is much slower, as a full optimisation for the scale
 parameters is done after each update of the alphas and beta.

 see: http://research.microsoft.com/mlp/RVM/relevance.htm

 Refs:
 The Relevance Vector Machine.
 In S. A. Solla, T. K. Leen, and K.-R. Müller (Eds.),
 Advances in Neural Information Processing Systems 12,
 pp.  652-658. Cambridge, Mass: MIT Press.

 Michael E. Tipping
 Sparse Bayesian Learning and the Relevance Vector Machine
 Journal of Machine Learning Research 1 (2001) 211-244
________________________________________________________________________
 Copyright (C) 2011 Wellcome Department of Imaging Neuroscience &amp; 
 Machine Learning &amp; Neuroimaging Laboratory</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../matlabicon.gif)">
</ul>
This function is called by:
<ul style="list-style-image:url(../matlabicon.gif)">
<li><a href="prt_machine_rvr.html" class="code" title="function output = prt_machine_rvr(d,args)">prt_machine_rvr</a>	Relevance vector regression (training and testing)</li></ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<ul style="list-style-image:url(../matlabicon.gif)">
<li><a href="#_sub1" class="code">function [w,alpha,beta,ll]=regression0(Phi,t)</a></li><li><a href="#_sub2" class="code">function [w,alpha,beta,ll] = rvr1a(Phi,t,alpha,beta)</a></li><li><a href="#_sub3" class="code">function [w,alpha,beta,ll]=rvr2a(Phi,t,alpha,beta)</a></li><li><a href="#_sub4" class="code">function [w,alpha,beta,nu,ll]=regression1(K,t,opt)</a></li><li><a href="#_sub5" class="code">function [w,alpha,beta,nu,ll] = rvr1(K,t,alpha,beta,nu,krn_f,dkrn_f)</a></li><li><a href="#_sub6" class="code">function [w,alpha,beta,nu,ll]=rvr2(K,t,alpha,beta,nu,krn_f,dkrn_f)</a></li><li><a href="#_sub7" class="code">function Phi = make_phi(nu,K,nz)</a></li><li><a href="#_sub8" class="code">function [dPhi,d2Phi] = make_dphi(nu,K,nz)</a></li><li><a href="#_sub9" class="code">function Phi = make_phi_rbf(nu,K,nz)</a></li><li><a href="#_sub10" class="code">function [dPhi,d2Phi] = make_dphi_rbf(nu,K,nz)</a></li><li><a href="#_sub11" class="code">function [nu,ll] = re_estimate_nu(K,t,nu,alpha,beta,krn_f,dkrn_f,nz)</a></li><li><a href="#_sub12" class="code">function [ld,C] = logdet(A)</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [varargout] = prt_rvr(varargin)</a>
0002 <span class="comment">% Optimisation for Relevance Vector Regression</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% [w,alpha,beta,ll] = prt_rvr(Phi,t)</span>
0005 <span class="comment">% Phi   - MxM matrix derived from kernel function of vector pairs</span>
0006 <span class="comment">% t     - the values to be matched</span>
0007 <span class="comment">% w     - weights</span>
0008 <span class="comment">% alpha - 1/variance for the prior part of the model</span>
0009 <span class="comment">% beta  - 1/variance for the likelihood part of the model</span>
0010 <span class="comment">% ll    - the negative log-likelihood.</span>
0011 <span class="comment">%</span>
0012 <span class="comment">% [w,alpha,beta,nu,ll]=spm_rvr(K,t,opt)</span>
0013 <span class="comment">% K     - a cell-array of MxM dot-product matrices.</span>
0014 <span class="comment">% t     - the values to be matched</span>
0015 <span class="comment">% opt   - either 'Linear' or 'Gaussian RBF'</span>
0016 <span class="comment">%         'Linear'       is for linear regression models, where</span>
0017 <span class="comment">%                        the optimal kernel is generated by</span>
0018 <span class="comment">%                        [nu(1)*K{1} + nu(1)*K{2}... ones(size(K{1},1),1)]</span>
0019 <span class="comment">%         'Gaussian RBF' is for regression using Gaussian radial basis</span>
0020 <span class="comment">%                        functions.  The kernel is generated from</span>
0021 <span class="comment">%                        P1  = nu(1)*K{1} + nu(1)*K{2} ... ;</span>
0022 <span class="comment">%                        P2  = repmat(diag(P1) ,1,size(P1,2)) +...</span>
0023 <span class="comment">%                              repmat(diag(P1)',size(P1,1),1) - 2*P1;</span>
0024 <span class="comment">%                        Phi = exp([-0.5*P2 ones(size(P1,1),1)]);</span>
0025 <span class="comment">% w     - weights</span>
0026 <span class="comment">% alpha - 1/variance for the prior part of the model</span>
0027 <span class="comment">% beta  - 1/variance for the likelihood part of the model</span>
0028 <span class="comment">% nu    - parameters that convert the dot-product matrices into</span>
0029 <span class="comment">%         a kernel matrix (Phi).</span>
0030 <span class="comment">% ll    - the negative log-likelihood.</span>
0031 <span class="comment">%</span>
0032 <span class="comment">% The first way of calling the routine simply optimises the</span>
0033 <span class="comment">% weights.  This involves estimating a restricted maximum</span>
0034 <span class="comment">% likelihood (REML) solution, which maximises P(alpha,beta|t,Phi).</span>
0035 <span class="comment">% Note that REML is also known as Type II Maximum Likelihood</span>
0036 <span class="comment">% (ML-II). The ML-II solution tends towards infinite weights for</span>
0037 <span class="comment">% some the regularisation terms (i.e. 1/alpha(i) approaches 0).</span>
0038 <span class="comment">% The appropriate columns are removed from the model when</span>
0039 <span class="comment">% this happens.</span>
0040 <span class="comment">%</span>
0041 <span class="comment">% The second way of calling the routine also estimates additional</span>
0042 <span class="comment">% input scale parameters as described in Appendix C of Tipping (2001).</span>
0043 <span class="comment">% This method is much slower, as a full optimisation for the scale</span>
0044 <span class="comment">% parameters is done after each update of the alphas and beta.</span>
0045 <span class="comment">%</span>
0046 <span class="comment">% see: http://research.microsoft.com/mlp/RVM/relevance.htm</span>
0047 <span class="comment">%</span>
0048 <span class="comment">% Refs:</span>
0049 <span class="comment">% The Relevance Vector Machine.</span>
0050 <span class="comment">% In S. A. Solla, T. K. Leen, and K.-R. Müller (Eds.),</span>
0051 <span class="comment">% Advances in Neural Information Processing Systems 12,</span>
0052 <span class="comment">% pp.  652-658. Cambridge, Mass: MIT Press.</span>
0053 <span class="comment">%</span>
0054 <span class="comment">% Michael E. Tipping</span>
0055 <span class="comment">% Sparse Bayesian Learning and the Relevance Vector Machine</span>
0056 <span class="comment">% Journal of Machine Learning Research 1 (2001) 211-244</span>
0057 <span class="comment">%________________________________________________________________________</span>
0058 <span class="comment">% Copyright (C) 2011 Wellcome Department of Imaging Neuroscience &amp;</span>
0059 <span class="comment">% Machine Learning &amp; Neuroimaging Laboratory</span>
0060 
0061 <span class="comment">% Written by John Ashburner</span>
0062 <span class="comment">% $Id$</span>
0063 
0064 <span class="keyword">if</span> isnumeric(varargin{1}),
0065     [varargout{1:nargout}]=<a href="#_sub1" class="code" title="subfunction [w,alpha,beta,ll]=regression0(Phi,t)">regression0</a>(varargin{:});
0066 <span class="keyword">elseif</span> iscell(varargin{1}),
0067     [varargout{1:nargout}]=<a href="#_sub4" class="code" title="subfunction [w,alpha,beta,nu,ll]=regression1(K,t,opt)">regression1</a>(varargin{:});
0068 <span class="keyword">else</span>
0069     error(<span class="string">'Incorrect usage'</span>);
0070 <span class="keyword">end</span>;
0071 <span class="keyword">return</span>;
0072 <span class="comment">%__________________________________________________________________________</span>
0073 
0074 <span class="comment">%__________________________________________________________________________</span>
0075 <a name="_sub1" href="#_subfunctions" class="code">function [w,alpha,beta,ll]=regression0(Phi,t)</a>
0076 [N,M]  = size(Phi);
0077 <span class="keyword">if</span> N==M,
0078     Phi = [Phi ones(N,1)];
0079 <span class="keyword">elseif</span> M~=N+1,
0080     error(<span class="string">'Phi must be N x (N+1)'</span>);
0081 <span class="keyword">end</span>;
0082 scale             = sqrt(sum(sum(Phi(1:N,1:N).^2))/N^2);
0083 scale             = [ones(N,1)*scale ; 1];
0084 Phi               = Phi/spdiags(scale,0,numel(scale),numel(scale));
0085 alpha             = ones(size(Phi,2),1)/N;
0086 <span class="comment">%beta             = N/sum((t-mean(t)).^2);</span>
0087 beta              = 1e6;
0088 [w,alpha,beta,ll] = <a href="#_sub2" class="code" title="subfunction [w,alpha,beta,ll] = rvr1a(Phi,t,alpha,beta)">rvr1a</a>(Phi,t,alpha,beta);
0089 alpha             = [alpha(1)*ones(N,1) ; alpha(2)];
0090 [w,alpha,beta,ll] = <a href="#_sub3" class="code" title="subfunction [w,alpha,beta,ll]=rvr2a(Phi,t,alpha,beta)">rvr2a</a>(Phi,t,alpha,beta);
0091 w                 = w./scale;
0092 alpha             = alpha.*scale.^2;
0093 <span class="keyword">return</span>;
0094 <span class="comment">%__________________________________________________________________________</span>
0095 
0096 <span class="comment">%__________________________________________________________________________</span>
0097 <a name="_sub2" href="#_subfunctions" class="code">function [w,alpha,beta,ll] = rvr1a(Phi,t,alpha,beta)</a>
0098 <span class="comment">% This function is not actually used</span>
0099 <span class="comment">%spm_chi2_plot('Init','ML-II (non-sparse)','-Log-likelihood','Iteration');</span>
0100 [N,M]   = size(Phi);
0101 ll      = Inf;
0102 PP      = Phi'*Phi;
0103 Pt      = Phi'*t;
0104 <span class="keyword">for</span> subit=1:10,
0105     alpha_old = alpha;
0106     beta_old  = beta;
0107 
0108     <span class="comment">% E-step</span>
0109     S         = inv(PP*beta + spdiags([ones(N,1)*alpha(1) ; alpha(2)],0,N+1,N+1));
0110     w         = S*(Pt*beta);
0111 
0112    <span class="comment">% figure(3); plot(t,Phi*w,'.'); drawnow;</span>
0113 
0114     tmp       = t-Phi*w;
0115     ll        = <span class="keyword">...</span>
0116          -0.5*log(alpha(1))*N-0.5*log(alpha(2))-0.5*N*log(beta)-0.5*<a href="#_sub12" class="code" title="subfunction [ld,C] = logdet(A)">logdet</a>(S)<span class="keyword">...</span>
0117          +0.5*tmp'*tmp*beta + 0.5*sum(w.^2.*[repmat(alpha(1),N,1) ; alpha(2)])<span class="keyword">...</span>
0118          +0.5*(M-N)*log(2*pi);
0119   <span class="comment">%  if subit&gt;1, spm_chi2_plot('Set',ll); end;</span>
0120     <span class="comment">%fprintf('%g\n',ll);</span>
0121 
0122     <span class="comment">% M-step</span>
0123     ds        = diag(S);
0124     dfa1      = sum(ds(1:N))*alpha(1);
0125     dfa2      = sum(ds(N+1))*alpha(2);
0126     alpha(1)  = max(N-dfa1,eps)/(sum(w(1:N).^2)   +eps);
0127     alpha(2)  = max(1-dfa2,eps)/(sum(w(N+1).^2)   +eps);
0128     beta      = max(dfa1+dfa2-1,eps)/(sum((Phi*w-t).^2)+eps);
0129 
0130     <span class="comment">% Convergence</span>
0131     <span class="keyword">if</span> max(max(abs(log((alpha+eps)./(alpha_old+eps)))),log(beta/beta_old)) &lt; 1e-9,
0132         <span class="keyword">break</span>;
0133     <span class="keyword">end</span>;
0134 <span class="keyword">end</span>;
0135 <span class="comment">%spm_chi2_plot('Clear');</span>
0136 <span class="keyword">return</span>;
0137 <span class="comment">%__________________________________________________________________________</span>
0138 
0139 <span class="comment">%__________________________________________________________________________</span>
0140 <a name="_sub3" href="#_subfunctions" class="code">function [w,alpha,beta,ll]=rvr2a(Phi,t,alpha,beta)</a>
0141 <span class="comment">%spm_chi2_plot('Init','ML-II (sparse)','-Log-likelihood','Iteration');</span>
0142 [N,M] = size(Phi);
0143 nz    = true(M,1);
0144 
0145 PP    = Phi'*Phi;
0146 Pt    = Phi'*t;
0147 
0148 <span class="keyword">for</span> subit=1:200,
0149     th         = min(alpha)*1e9;
0150     nz         = alpha&lt;th;
0151     alpha(~nz) = th*1e9;
0152     alpha_old  = alpha;
0153     beta_old   = beta;
0154 
0155     <span class="comment">% E-step</span>
0156     S         = inv(PP(nz,nz)*beta + diag(alpha(nz)));
0157     w         = S*Pt(nz)*beta;
0158 
0159   <span class="comment">%  figure(3); plot(t,Phi(:,nz)*w,'.'); drawnow;</span>
0160 
0161     tmp = t-Phi(:,nz)*w;
0162     ll  = <span class="keyword">...</span>
0163         -0.5*sum(log(alpha(nz)+1e-32))-0.5*N*log(beta+1e-32)-0.5*<a href="#_sub12" class="code" title="subfunction [ld,C] = logdet(A)">logdet</a>(S)<span class="keyword">...</span>
0164         +0.5*tmp'*tmp*beta + 0.5*sum(w.^2.*alpha(nz))<span class="keyword">...</span>
0165         +0.5*(sum(nz)-N)*log(2*pi);
0166 <span class="comment">%    if subit&gt;0, spm_chi2_plot('Set',ll); end;</span>
0167     <span class="comment">%fprintf('%g\t%g\n',ll,exp(mean(log(alpha)))/beta);</span>
0168 
0169     <span class="comment">% M-step</span>
0170     gam       = 1 - alpha(nz).*diag(S);
0171     alpha(nz) = max(gam,eps)./(w.^2+1e-32);
0172     beta      = max(N-sum(gam),eps)./(sum((Phi(:,nz)*w-t).^2)+1e-32);
0173 
0174     <span class="comment">% Convergence</span>
0175     <span class="keyword">if</span> max(max(abs(log((alpha(nz)+eps)./(alpha_old(nz)+eps)))),log(beta/beta_old)) &lt; 1e-6*N,
0176         <span class="keyword">break</span>;
0177     <span class="keyword">end</span>;
0178 <span class="keyword">end</span>;
0179 w(nz)  = w;
0180 w(~nz) = 0;
0181 w      = w(:);
0182 <span class="comment">%spm_chi2_plot('Clear');</span>
0183 <span class="comment">%__________________________________________________________________________</span>
0184 
0185 <span class="comment">%__________________________________________________________________________</span>
0186 <a name="_sub4" href="#_subfunctions" class="code">function [w,alpha,beta,nu,ll]=regression1(K,t,opt)</a>
0187 <span class="comment">% Relevance vector regression</span>
0188 <span class="keyword">if</span> nargin&lt;3, opt = <span class="string">'Linear'</span>; <span class="keyword">end</span>;
0189 <span class="keyword">switch</span> opt,
0190 <span class="keyword">case</span> {<span class="string">'Linear'</span>,<span class="string">'linear'</span>,<span class="string">'lin'</span>},
0191     dkrn_f = @<a href="#_sub8" class="code" title="subfunction [dPhi,d2Phi] = make_dphi(nu,K,nz)">make_dphi</a>;
0192     krn_f  = @<a href="#_sub7" class="code" title="subfunction Phi = make_phi(nu,K,nz)">make_phi</a>;
0193 <span class="keyword">case</span> {<span class="string">'Gaussian RBF'</span>,<span class="string">'nonlinear'</span>,<span class="string">'nonlin'</span>},
0194     dkrn_f = @<a href="#_sub10" class="code" title="subfunction [dPhi,d2Phi] = make_dphi_rbf(nu,K,nz)">make_dphi_rbf</a>;
0195     krn_f  = @<a href="#_sub9" class="code" title="subfunction Phi = make_phi_rbf(nu,K,nz)">make_phi_rbf</a>;
0196 <span class="keyword">otherwise</span>
0197     error(<span class="string">'Unknown option'</span>);
0198 <span class="keyword">end</span>;
0199 [N,M]  = size(K{1});
0200 nu     = ones(numel(K),1);
0201 rescal = ones(numel(K),1);
0202 <span class="keyword">for</span> i=1:numel(K),
0203     <span class="keyword">if</span> strcmpi(opt,<span class="string">'Gaussian RBF'</span>) || strcmpi(opt,<span class="string">'nonlinear'</span>) || strcmpi(opt,<span class="string">'nonlin'</span>),
0204         d         = 0.5*diag(K{i});
0205         K{i}      = repmat(d,[1 size(K{i},1)]) + repmat(d',[size(K{i},1),1]) - K{i};
0206         K{i}      = max(K{i},0);
0207         K{i}      = -K{i};
0208         nu(i)     = 1/sqrt(sum(K{i}(:).^2)/(size(K{i},1).^2-size(K{i},1)));
0209     <span class="keyword">else</span>
0210         rescal(i) = sqrt(size(K{i},1)/sum(K{i}(:).^2));
0211         K{i}      = K{i}.*rescal(i);
0212     <span class="keyword">end</span>;
0213 <span class="keyword">end</span>;
0214 
0215 alpha  = [1 1]';
0216 <span class="comment">%beta  = 1/sum((t-mean(t)).^2);</span>
0217 beta   = 1e6;
0218 [w,alpha,beta,nu,ll]=<a href="#_sub5" class="code" title="subfunction [w,alpha,beta,nu,ll] = rvr1(K,t,alpha,beta,nu,krn_f,dkrn_f)">rvr1</a>(K,t,alpha,beta,nu,krn_f,dkrn_f);
0219 alpha  = [alpha(1)*ones(N,1) ; alpha(2)];
0220 [w,alpha,beta,nu,ll]=<a href="#_sub6" class="code" title="subfunction [w,alpha,beta,nu,ll]=rvr2(K,t,alpha,beta,nu,krn_f,dkrn_f)">rvr2</a>(K,t,alpha,beta,nu,krn_f,dkrn_f);
0221 nu    = nu.*rescal;
0222 <span class="keyword">return</span>;
0223 <span class="comment">%__________________________________________________________________________</span>
0224 
0225 <span class="comment">%__________________________________________________________________________</span>
0226 <a name="_sub5" href="#_subfunctions" class="code">function [w,alpha,beta,nu,ll] = rvr1(K,t,alpha,beta,nu,krn_f,dkrn_f)</a>
0227 <span class="comment">% This function is not actually used</span>
0228 spm_chi2_plot(<span class="string">'Init'</span>,<span class="string">'ML-II (non-sparse)'</span>,<span class="string">'-Log-likelihood'</span>,<span class="string">'Iteration'</span>);
0229 [N,M]   = size(K{1});
0230 ll      = Inf;
0231 <span class="keyword">for</span> iter=1:50,
0232     Phi     = feval(krn_f,nu,K);
0233     <span class="keyword">for</span> subit=1:1,
0234         alpha_old = alpha;
0235         beta_old  = beta;
0236 
0237         <span class="comment">% E-step</span>
0238         S         = inv(Phi'*Phi*beta + spdiags([ones(N,1)*alpha(1) ; alpha(2)],0,N+1,N+1));
0239         w         = S*(Phi'*t*beta);
0240 
0241         <span class="comment">% M-step</span>
0242         ds        = diag(S);
0243         dfa1      = sum(ds(1:N))*alpha(1);
0244         dfa2      = sum(ds(N+1))*alpha(2);
0245         alpha(1)  = max(N-dfa1,eps)/(sum(w(1:N).^2)   +eps);
0246         alpha(2)  = max(1-dfa2,eps)/(sum(w(N+1).^2)   +eps);
0247         beta      = max(dfa1+dfa2-1,eps)/(sum((Phi*w-t).^2)+eps);
0248 
0249         <span class="comment">% Convergence</span>
0250         <span class="keyword">if</span> max(max(abs(log((alpha+eps)./(alpha_old+eps)))),log(beta/beta_old)) &lt; 1e-9,
0251             <span class="keyword">break</span>;
0252         <span class="keyword">end</span>;
0253     <span class="keyword">end</span>;
0254 
0255 
0256     <span class="comment">% Update nu</span>
0257     oll       = ll;
0258     al1       = [ones(N,1)*alpha(1) ; alpha(2)];
0259     [nu,ll]   = <a href="#_sub11" class="code" title="subfunction [nu,ll] = re_estimate_nu(K,t,nu,alpha,beta,krn_f,dkrn_f,nz)">re_estimate_nu</a>(K,t,nu,al1,beta,krn_f,dkrn_f);
0260 
0261 <span class="comment">%    scale = sqrt(sum(nu.^2));</span>
0262 <span class="comment">%    nu    = nu/scale;</span>
0263 <span class="comment">%    alpha = alpha/scale^2;</span>
0264 
0265     spm_chi2_plot(<span class="string">'Set'</span>,ll);
0266     <span class="keyword">if</span> abs(oll-ll) &lt; 1e-6*N, <span class="keyword">break</span>; <span class="keyword">end</span>;
0267 <span class="keyword">end</span>;
0268 spm_chi2_plot(<span class="string">'Clear'</span>);
0269 <span class="keyword">return</span>;
0270 <span class="comment">%__________________________________________________________________________</span>
0271 
0272 <span class="comment">%__________________________________________________________________________</span>
0273 <a name="_sub6" href="#_subfunctions" class="code">function [w,alpha,beta,nu,ll]=rvr2(K,t,alpha,beta,nu,krn_f,dkrn_f)</a>
0274 spm_chi2_plot(<span class="string">'Init'</span>,<span class="string">'ML-II (sparse)'</span>,<span class="string">'-Log-likelihood'</span>,<span class="string">'Iteration'</span>);
0275 [N,M] = size(K{1});
0276 w     = zeros(N+1,1);
0277 ll    = Inf;
0278 <span class="keyword">for</span> iter=1:100,
0279     <span class="keyword">for</span> subits=1:1,
0280         <span class="comment">% Suboptimal estimates of nu if the alphas and weights are pruned</span>
0281         <span class="comment">% too quickly.</span>
0282 
0283         th         = min(alpha)*1e9;
0284         nz         = alpha&lt;th;
0285         alpha(~nz) = th*1e9;
0286 
0287         alpha_old  = alpha;
0288         beta_old   = beta;
0289         Phi        = feval(krn_f,nu,K,nz);
0290 
0291         <span class="comment">% E-step</span>
0292         S         = inv(beta*Phi'*Phi + diag(alpha(nz)));
0293         w(nz)     = S*Phi'*t*beta;
0294         w(~nz)    = 0;
0295 
0296         <span class="comment">% figure(3); plot(t,Phi*w(nz),'.');drawnow;</span>
0297 
0298         <span class="comment">% M-step</span>
0299         gam       = 1 - alpha(nz).*diag(S);
0300         alpha(nz) = max(gam,eps)./(w(nz).^2+1e-32);
0301         beta      = max(N-sum(gam),eps)./(sum((Phi*w(nz)-t).^2)+1e-32);
0302 
0303        <span class="comment">% Convergence</span>
0304         <span class="keyword">if</span> max(max(abs(log((alpha+eps)./(alpha_old+eps)))),log(beta/beta_old)) &lt; 1e-6,
0305             <span class="keyword">break</span>;
0306         <span class="keyword">end</span>;
0307     <span class="keyword">end</span>;
0308 
0309     oll       = ll;
0310     [nu,ll]   = <a href="#_sub11" class="code" title="subfunction [nu,ll] = re_estimate_nu(K,t,nu,alpha,beta,krn_f,dkrn_f,nz)">re_estimate_nu</a>(K,t,nu,alpha,beta,krn_f,dkrn_f,nz);
0311 
0312     <span class="comment">% scale     = sqrt(sum(nu.^2));</span>
0313     <span class="comment">% nu        = nu/scale;</span>
0314     <span class="comment">% alpha     = alpha/scale^2;</span>
0315 
0316     spm_chi2_plot(<span class="string">'Set'</span>,ll);
0317 
0318     <span class="comment">% Convergence</span>
0319     <span class="keyword">if</span> abs(oll-ll) &lt; 1e-9*N,
0320         <span class="keyword">break</span>;
0321     <span class="keyword">end</span>;
0322 <span class="keyword">end</span>;
0323 spm_chi2_plot(<span class="string">'Clear'</span>);
0324 <span class="keyword">return</span>;
0325 <span class="comment">%__________________________________________________________________________</span>
0326 
0327 <span class="comment">%__________________________________________________________________________</span>
0328 <a name="_sub7" href="#_subfunctions" class="code">function Phi = make_phi(nu,K,nz)</a>
0329 <span class="comment">% Dot product matrix, generated from linear combination of dot-product</span>
0330 <span class="comment">% matrices.</span>
0331 <span class="keyword">if</span> nargin&gt;2 &amp;&amp; ~isempty(nz),
0332     nz1 = nz(1:size(K{1},1));
0333     nz2 = nz(size(K{1},1)+1);
0334     Phi = K{1}(:,nz1)*nu(1);
0335     <span class="keyword">for</span> i=2:numel(K),
0336         Phi=Phi+K{i}(:,nz1)*nu(i);
0337     <span class="keyword">end</span>;
0338     Phi = [Phi ones(size(Phi,1),sum(nz2))];
0339 <span class="keyword">else</span>
0340     Phi = K{1}*nu(1);
0341     <span class="keyword">for</span> i=2:numel(K),
0342         Phi=Phi+K{i}*nu(i);
0343     <span class="keyword">end</span>;
0344     Phi = [Phi ones(size(Phi,1),1)];
0345 <span class="keyword">end</span>;
0346 <span class="keyword">return</span>;
0347 <span class="comment">%__________________________________________________________________________</span>
0348 
0349 <span class="comment">%__________________________________________________________________________</span>
0350 <a name="_sub8" href="#_subfunctions" class="code">function [dPhi,d2Phi] = make_dphi(nu,K,nz)</a>
0351 <span class="comment">% First and second derivatives of Phi with respect to nu, where</span>
0352 <span class="comment">% Phi is a dot-product matrix.</span>
0353 dPhi  = cell(size(K));
0354 d2Phi = cell(numel(K));
0355 <span class="keyword">if</span> nargin&gt;2 &amp;&amp; ~isempty(nz),
0356     nz1 = nz(1:size(K{1},1));
0357     nz2 = nz(size(K{1},1)+1);
0358     <span class="keyword">for</span> i=1:numel(K),
0359         dPhi{i} = [K{i}(:,nz1),zeros(size(K{i},1),sum(nz2))];
0360         dPhi{i} = dPhi{i}*nu(i);
0361     <span class="keyword">end</span>;
0362 <span class="keyword">else</span>
0363     <span class="keyword">for</span> i=1:numel(K),
0364         dPhi{i} = [K{i},zeros(size(K{i},1),1)];
0365         dPhi{i} = dPhi{i}*nu(i);
0366     <span class="keyword">end</span>;
0367 <span class="keyword">end</span>;
0368 <span class="comment">%z = sparse([],[],[],size(dPhi{1},1),size(dPhi{1},2));</span>
0369 z = zeros(size(dPhi{1}));
0370 <span class="keyword">for</span> i=1:numel(K),
0371     d2Phi{i,i} = nu(i)*dPhi{i};
0372     <span class="keyword">for</span> j=(i+1):numel(K),
0373         d2Phi{i,j} = z;
0374         d2Phi{j,i} = z;
0375     <span class="keyword">end</span>;
0376     dPhi{i} = nu(i)*dPhi{i};
0377 <span class="keyword">end</span>;
0378 <span class="keyword">return</span>;
0379 <span class="comment">%__________________________________________________________________________</span>
0380 
0381 <span class="comment">%__________________________________________________________________________</span>
0382 <a name="_sub9" href="#_subfunctions" class="code">function Phi = make_phi_rbf(nu,K,nz)</a>
0383 <span class="comment">% Radial basis function kernel.</span>
0384 <span class="keyword">if</span> nargin&gt;2 &amp;&amp; ~isempty(nz),
0385     nz1 = nz(1:size(K{1},1));
0386     nz2 = nz(size(K{1},1)+1);
0387     Phi       = K{1}(:,nz1)*nu(1);
0388     <span class="keyword">for</span> i=2:numel(K),
0389         Phi=Phi+K{i}(:,nz1)*nu(i);
0390     <span class="keyword">end</span>;
0391     Phi = [exp(Phi) ones(size(Phi,1),sum(nz2))];
0392 <span class="keyword">else</span>
0393     Phi       = K{1}*nu(1);
0394     <span class="keyword">for</span> i=2:numel(K),
0395         Phi=Phi+K{i}*nu(i);
0396     <span class="keyword">end</span>;
0397     Phi = [exp(Phi) ones(size(Phi,1),1)];
0398 <span class="keyword">end</span>;
0399 <span class="keyword">return</span>;
0400 <span class="comment">%__________________________________________________________________________</span>
0401 
0402 <span class="comment">%__________________________________________________________________________</span>
0403 <a name="_sub10" href="#_subfunctions" class="code">function [dPhi,d2Phi] = make_dphi_rbf(nu,K,nz)</a>
0404 <span class="comment">% First and second derivatives of Phi with respect to nu, where</span>
0405 <span class="comment">% Phi is defined by radial basis functions.</span>
0406 Phi   = <a href="#_sub9" class="code" title="subfunction Phi = make_phi_rbf(nu,K,nz)">make_phi_rbf</a>(nu,K,nz);
0407 dPhi  = cell(size(K));
0408 d2Phi = cell(numel(K));
0409 <span class="keyword">if</span> nargin&gt;2 &amp;&amp; ~isempty(nz),
0410     nz1 = nz(1:size(K{1},1));
0411     nz2 = nz(size(K{1},1)+1);
0412     <span class="keyword">for</span> i=1:numel(K),
0413         dPhi{i} = [K{i}(:,nz1),zeros(size(K{i},1),sum(nz2))];
0414     <span class="keyword">end</span>;
0415 <span class="keyword">else</span>
0416     <span class="keyword">for</span> i=1:numel(K),
0417         dPhi{i} = [K{i},zeros(size(K{i},1),1)];
0418     <span class="keyword">end</span>;
0419 <span class="keyword">end</span>;
0420 
0421 <span class="keyword">for</span> i=1:numel(K),
0422     d2Phi{i,i} = nu(i)*dPhi{i}.*Phi.*(1+nu(i)*dPhi{i});
0423     <span class="keyword">for</span> j=(i+1):numel(K),
0424         d2Phi{i,j} = (nu(i)*nu(j))*dPhi{i}.*dPhi{j}.*Phi;
0425         d2Phi{j,i} = d2Phi{i,j};
0426     <span class="keyword">end</span>;
0427     dPhi{i} = nu(i)*dPhi{i}.*Phi;
0428 <span class="keyword">end</span>;
0429 <span class="keyword">return</span>;
0430 <span class="comment">%__________________________________________________________________________</span>
0431 
0432 <span class="comment">%__________________________________________________________________________</span>
0433 <a name="_sub11" href="#_subfunctions" class="code">function [nu,ll] = re_estimate_nu(K,t,nu,alpha,beta,krn_f,dkrn_f,nz)</a>
0434 <span class="comment">% See Appendix C of Tipping (2001).  Note that a Levenberg-Marquardt</span>
0435 <span class="comment">% approach is used for the optimisation.</span>
0436 <span class="keyword">if</span> nargin&lt;8, nz = true(size(K{1},2)+1,1); <span class="keyword">end</span>;
0437 
0438 ll   = Inf;
0439 lam  = 1e-6;
0440 Phi  = feval(krn_f,nu,K,nz);
0441 S    = inv(Phi'*Phi*beta+diag(alpha(nz)));
0442 w    = beta*S*Phi'*t;
0443 N    = size(Phi,1);
0444 ll   = <span class="keyword">...</span>
0445      -0.5*sum(log(alpha(nz)))-0.5*N*log(beta)-0.5*<a href="#_sub12" class="code" title="subfunction [ld,C] = logdet(A)">logdet</a>(S)<span class="keyword">...</span>
0446      +0.5*(t-Phi*w)'*(t-Phi*w)*beta + 0.5*sum(w.^2.*alpha(nz))<span class="keyword">...</span>
0447      +0.5*(sum(nz)-N)*log(2*pi);
0448 
0449 <span class="keyword">for</span> subit=1:30,
0450 
0451     <span class="comment">% Generate 1st and second derivatives of the objective function (ll).</span>
0452     <span class="comment">% These are derived from the first and second partial derivatives</span>
0453     <span class="comment">% of Phi with respect to nu.</span>
0454     g    = zeros(numel(K),1);
0455     H    = zeros(numel(K));
0456     [dPhi,d2Phi] = feval(dkrn_f,nu,K,nz);
0457     <span class="keyword">for</span> i=1:numel(K),
0458         tmp1  = Phi'*dPhi{i};
0459         tmp1  = tmp1+tmp1';
0460         g(i)  = 0.5*beta*(sum(sum(S.*tmp1)) + w'*tmp1*w - 2*t'*dPhi{i}*w);
0461         <span class="keyword">for</span> j=i:numel(K),
0462             tmp    = dPhi{j}'*dPhi{i} + Phi'*d2Phi{i,j};
0463             tmp    = tmp+tmp';
0464             tmp2   = Phi'*dPhi{j};
0465             tmp2   = tmp2+tmp2';
0466             H(i,j) = sum(sum(S.*(tmp - tmp1*S*tmp2))) + w'*tmp*w - 2*w'*d2Phi{i,j}'*t;
0467             H(i,j) = 0.5*beta*H(i,j);
0468             H(j,i) = H(i,j);
0469         <span class="keyword">end</span>;
0470     <span class="keyword">end</span>;
0471 
0472     oll  = ll;
0473     onu  = nu;
0474 
0475     <span class="comment">% Negative eigenvalues indicate that the solution is tending</span>
0476     <span class="comment">% towards a saddle point or a maximum rather than a minimum</span>
0477     <span class="comment">% of the negative log-likelihood</span>
0478     lam  = max(lam,-real(min(eig(H)))*1.5);
0479 
0480     <span class="keyword">for</span> subsubit=1:30,
0481         drawnow;
0482 
0483         <span class="comment">% Levenberg-Marquardt update of log(nu)</span>
0484         warning off
0485         nu        = exp(log(nu) - (H+lam*speye(size(H)))\g);
0486         warning on
0487 
0488         <span class="comment">% Make sure the values are within a semi-reasonable range</span>
0489         nu        = max(max(nu,1e-12),max(nu)*1e-9);
0490         nu        = min(min(nu,1e12) ,min(nu)*1e9);
0491 
0492         <span class="comment">% Re-compute the log-likelihood</span>
0493         Phi1      = feval(krn_f,nu,K,nz);
0494         warning off
0495         S1        = inv(Phi1'*Phi1*beta+diag(alpha(nz))); <span class="comment">% Sigma</span>
0496         warning on
0497         w1        = beta*S1*Phi1'*t; <span class="comment">% weights</span>
0498         ll        = <span class="keyword">...</span>
0499              -0.5*sum(log(alpha(nz)+eps))-0.5*N*log(beta+eps)-0.5*<a href="#_sub12" class="code" title="subfunction [ld,C] = logdet(A)">logdet</a>(S1)<span class="keyword">...</span>
0500              +0.5*(t-Phi1*w1)'*(t-Phi1*w1)*beta + 0.5*sum(w1.^2.*alpha(nz))<span class="keyword">...</span>
0501              +0.5*(sum(nz)-N)*log(2*pi);
0502 
0503         <span class="comment">% Terminate if no difference</span>
0504         <span class="keyword">if</span> abs(ll-oll)&lt;1e-9,
0505             nu  = onu;
0506             ll  = oll;
0507             <span class="keyword">break</span>;
0508         <span class="keyword">end</span>;
0509 
0510         <span class="keyword">if</span> ll&gt;oll, <span class="comment">% Solution gets worse</span>
0511             lam = lam*10; <span class="comment">% More regularisation required</span>
0512             nu  = onu; <span class="comment">% Discard new estimates of nu and ll</span>
0513             ll  = oll;
0514         <span class="keyword">else</span> <span class="comment">% Solution improves</span>
0515             lam = lam/10; <span class="comment">% Try even less regularisation</span>
0516             lam = max(lam,1e-12);
0517 
0518             <span class="comment">% Use the new Phi, S and w for recomputing the</span>
0519             <span class="comment">% derivatives in the next iteration</span>
0520             Phi = Phi1;
0521             S   = S1;
0522             w   = w1;
0523             <span class="keyword">break</span>;
0524         <span class="keyword">end</span>;
0525     <span class="keyword">end</span>;
0526     <span class="keyword">if</span> abs(ll-oll)&lt;1e-9*N, <span class="keyword">break</span>; <span class="keyword">end</span>;
0527 <span class="keyword">end</span>;
0528 <span class="comment">%__________________________________________________________________________</span>
0529 
0530 <span class="comment">%__________________________________________________________________________</span>
0531 <a name="_sub12" href="#_subfunctions" class="code">function [ld,C] = logdet(A)</a>
0532 A  = (A+A')/2;
0533 C  = chol(A);
0534 d  = max(diag(C),eps);
0535 ld = sum(2*log(d));
0536</pre></div>
<hr><address>Generated on Tue 10-Feb-2015 18:16:33 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>